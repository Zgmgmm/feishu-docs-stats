import asyncio
import logging
import json
from typing import *
from lark_oapi.api.wiki.v2 import *
from dataclasses import dataclass, field
from urllib.parse import urlparse
import lark_oapi as lark
from doc_stats_utils import client, parse_lark_url, logger, batch_get_stats_async, get_node_space_id_async
from itertools import tee
from lark_oapi.api.drive.v1.model import MetaRequest, RequestDoc, Meta
from lark_oapi.api.drive.v1.resource.meta import BatchQueryMetaRequest, BatchQueryMetaResponse

from typing import Callable


logger = logging.getLogger('doc-stats')


fake_tree = {
    "A": ["B", "C", "D", "E"],
    "B": ["F", "G", "H", "I"],
    "C": ["J", "K", "L", "M"],
    "D": ["N", "O", "P", "Q"],
    "E": ["R", "S", "T", "U"],
    "F": ["V"],
    "G": ["W"],
    "H": ["X"],
    "I": ["Y"],
    "J": ["Z"],
    "K": ["AA"],
    "L": ["AB"],
    "M": ["AC"],
    "N": ["AD"],
    "O": ["AE"],
    "P": ["AF"],
    "Q": [],
    "R": [],
    "S": [],
    "T": [],
    "U": [],
    "V": [],
    "W": [],
    "X": [],
    "Y": [],
    "Z": [],
    "AA": [],
    "AB": [],
    "AC": [],
    "AD": [],
    "AE": [],
    "AF": [],
}

# æ¨¡æ‹Ÿå¼‚æ­¥è·å–å­èŠ‚ç‚¹çš„å‡½æ•°
async def mock_get_children(node: Node) -> List[Node]:
    await asyncio.sleep(random.uniform(1, 3))  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    print(f"è·å– {node} çš„å­èŠ‚ç‚¹ï¼š{fake_tree.get(node, [])}")
    tokens = fake_tree.get(node, [])
    return [NodeBuilder().node_token(token).build() for token in tokens]

def has_children(node):
    return len(fake_tree.get(node, [])) > 0


from asyncio_throttle import Throttler  # å¯¼å…¥ç°æˆé™æµå™¨

# âœ… æ”¯æŒæµå¼è¾“å‡ºçš„å¹¶å‘ BFSï¼ˆé€‚ç”¨äºæ ‘ç»“æ„ï¼‰
async def walk_tree_concurrent(roots: list[Node], throttler: Throttler, user_token: str) -> Node:
    q_nodes = asyncio.Queue[Node]()
    q_parents = asyncio.Queue[Node]()

    async def worker():
        node = await q_parents.get()
        await q_nodes.put(node)
        # APIè°ƒç”¨å‰è¿›è¡ŒQPSé™æµ
        async with throttler:
            children = await get_children(node, user_token)  # å®é™…æ¥å£è°ƒç”¨
            for child in children:
                if child.has_child:
                    await q_parents.put(child)
                    asyncio.create_task(worker())
                else:
                    await q_nodes.put(child)
        q_parents.task_done()
        
    async def add_parent(node: Node):
        await q_parents.put(node)
        asyncio.create_task(worker())

    async def wait_for_completion():
        await q_parents.join()
        await q_nodes.put(None)

    for root in roots:
        await add_parent(root)
        
    asyncio.create_task(wait_for_completion())

    # ğŸ‘‡ ç”¨ async generator æŒç»­ yield æµå¼æ•°æ®
    while True:
        item = await q_nodes.get()
        if item is None:
            break
        yield item

# è¿™æ˜¯æ‰¹é‡æ”¶é›†å™¨ï¼Œå¼‚æ­¥è¿­ä»£å™¨è¾“å…¥ï¼Œæ‰¹é‡è¾“å‡º list
async def batcher(iter: Iterator[Node], batch_size: int) -> List[Node]:
    batch:List[Node] = []
    async for item in iter:
        batch.append(item)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    # æœ€åä¸€æ‰¹ä¸è¶³ batch_sizeï¼Œä¹Ÿè¾“å‡º
    if batch[0]:
        yield batch


async def batch_get_meta_async(docs: List[RequestDoc], user_token: str = None) -> List[Meta]:
    """è·å–å•ä¸ªæ‰¹æ¬¡çš„æ–‡æ¡£å…ƒæ•°æ®ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
    
    Args:
        docs: RequestDocå¯¹è±¡åˆ—è¡¨ï¼ˆä¸è¶…è¿‡200ä¸ªï¼‰
        user_token: ç”¨æˆ·token
        
    Returns:
        å…ƒæ•°æ®åˆ—è¡¨
    """
    if not docs:
        return []
    
    if len(docs) > 200:
        logger.warning(f"å•ä¸ªæ‰¹æ¬¡æ–‡æ¡£æ•°é‡è¶…è¿‡200ä¸ªé™åˆ¶: {len(docs)}ï¼Œå°†æˆªå–å‰200ä¸ª")
        docs = docs[:200]
    
    # æ„é€ è¯·æ±‚å¯¹è±¡
    request: BatchQueryMetaRequest = BatchQueryMetaRequest.builder() \
        .user_id_type("open_id") \
        .request_body(MetaRequest.builder()
            .request_docs(docs)
            .with_url(False)
            .build()) \
        .build()

    try:
        options = lark.RequestOption.builder().user_access_token(user_token).build()
        response: BatchQueryMetaResponse = await client.drive.v1.meta.abatch_query(request, options)
        
        # å¤„ç†å¤±è´¥è¿”å›
        if not response.success():
            error_msg = f"client.drive.v1.meta.batch_query failed, code: {response.code}, msg: {response.msg}, log_id: {response.get_log_id()}"
            if response.raw and response.raw.content:
                try:
                    error_detail = json.loads(response.raw.content)
                    error_msg += f", resp: \n{json.dumps(error_detail, indent=4, ensure_ascii=False)}"
                except:
                    error_msg += f", raw content: {response.raw.content}"
            
            lark.logger.error(error_msg)
            logger.error(f"æ‰¹é‡è·å–å…ƒæ•°æ®å¤±è´¥: {response.code} - {response.msg}")
            return []
            
        # è¿”å›å…ƒæ•°æ®
        if response.data and response.data.metas:
            logger.debug(f"æˆåŠŸè·å– {len(response.data.metas)} ä¸ªå…ƒæ•°æ®")
            return response.data.metas
        else:
            logger.warning("APIè¿”å›æˆåŠŸä½†æœªè·å–åˆ°å…ƒæ•°æ®")
            return []
            
    except Exception as e:
        logger.exception(f"æ‰¹é‡è·å–å…ƒæ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return []

async def get_wiki_node(token: str, user_token: str) -> Optional[Node]:
    """è·å–æ ¹èŠ‚ç‚¹ä¿¡æ¯
    
    Args:
        root_node_token: æ ¹èŠ‚ç‚¹token
        
    Returns:
        æ ¹èŠ‚ç‚¹ä¿¡æ¯å¯¹è±¡
    """
    node_info_request = GetNodeSpaceRequest.builder().token(token).obj_type("wiki").build()
    node_info_options = lark.RequestOption.builder().user_access_token(user_token).build()
    
    try:
        node_info_resp = await client.wiki.v2.space.aget_node(node_info_request, node_info_options)
        if not node_info_resp.success() or not node_info_resp.data or not node_info_resp.data.node:
            logger.error(f"æ— æ³•è·å–æ ¹èŠ‚ç‚¹ä¿¡æ¯: {root_node_ttokenoken}")
            return None
            
        return node_info_resp.data.node
    except Exception as e:
        logger.exception(f"è·å–æ ¹èŠ‚ç‚¹ä¿¡æ¯æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return None


home = Node({
    "creator": "ou_12281da53242d092b4f94fd05a8db87d",
    "has_child": True,
    "node_create_time": "1720163737",
    "node_creator": "ou_12281da53242d092b4f94fd05a8db87d",
    "node_token": "GQ0Owf7EmirsookHOh4cpx4XnMf",
    "node_type": "origin",
    "obj_create_time": "1720163737",
    "obj_edit_time": "1749977983",
    "obj_token": "QVIndcvtbocvZjxaOuJcgid5nFe",
    "obj_type": "docx",
    "origin_node_token": "GQ0Owf7EmirsookHOh4cpx4XnMf",
    "origin_space_id": "7064927739869954076",
    "owner": "ou_12281da53242d092b4f94fd05a8db87d",
    "parent_node_token": "wikcn6YQbanr7yHT1pucnHeewcc",
    "space_id": "7064927739869954076",
    "title": "é¦–é¡µæ¡†æ¶"
})
node = Node({
    "creator": "ou_12281da53242d092b4f94fd05a8db87d",
    "has_child": True,
    "node_create_time": "1735452354",
    "node_creator": "ou_12281da53242d092b4f94fd05a8db87d",
    "node_token": "R7oQwIXvUixG3FkimMBcTx2Inxh",
    "node_type": "origin",
    "obj_create_time": "1735452354",
    "obj_edit_time": "1736610706",
    "obj_token": "Jv1ndYk7DoUP4jxnI3ZcGfjqnSc",
    "obj_type": "docx",
    "origin_node_token": "R7oQwIXvUixG3FkimMBcTx2Inxh",
    "origin_space_id": "7064927739869954076",
    "owner": "ou_12281da53242d092b4f94fd05a8db87d",
    "parent_node_token": "GQ0Owf7EmirsookHOh4cpx4XnMf",
    "space_id": "7064927739869954076",
    "title": "æ€»ç»“&è§„åˆ’"
})
async def get_wiki_info(tokens: List[str], user_token: str):
    roots = [await get_wiki_node(token, user_token) for token in tokens]
    async_gen = walk_tree_concurrent(roots, throttler, user_token)
    res = []
    async for batch in batcher(async_gen, batch_size = 200):
        docs = [RequestDoc.builder().doc_token(doc.obj_token).doc_type(doc.obj_type).build() for doc in batch]
        infos = await get_doc_info(docs, user_token)
        res += infos
        # files = [(node.obj_token, node.obj_type) for node in batch]
        # stats_task = batch_get_stats_async(files, user_token)
        # docs = [RequestDoc.builder().doc_token(doc.obj_token).doc_type(doc.obj_type).build() for doc in batch]
        # meta_task = batch_get_meta_async(docs, user_token)
        # stats_dict, metas = await asyncio.gather(stats_task, meta_task)
        # meta_dict = {meta.doc_token: meta for meta in metas}
        # for node in batch: 
        #     token = node.obj_token
        #     stat = stats_dict[(node.obj_token, node.obj_type)]
        #     meta = meta_dict[token]
        #     res.append({
        #         "title": meta.title,
        #         "type": meta.doc_type,
        #         "token": meta.doc_token,
        #         "node_token": node.node_token,
        #         "source_url": f"https://docs.feishu.cn/{meta.doc_type}/{meta.doc_token}",
        #         "uv": stat.uv,
        #         "pv": stat.pv,
        #         "like_count": max(stat.like_count,0),
        #         "timestamp": meta.latest_modify_time,
        #         "uv_today": stat.uv_today,
        #         "pv_today": stat.pv_today,
        #         "like_count_today": stat.like_count_today,
        #         "update_time": meta.latest_modify_time,
        #     })
        #     i += 1
        #     print(f"Doc {i} {vars(meta)}ï¼š{vars(stat)}")
        # print(f"æ‰¹é‡å¤„ç†ï¼š{batch}")
        # è¿™é‡Œå¯ä»¥ç”¨ await è°ƒç”¨æ‰¹é‡æ¥å£ã€æ‰¹é‡æ•°æ®åº“å†™å…¥ç­‰
    return res
async def get_doc_info(docs: List[RequestDoc], user_token: str):
    files = [(doc.doc_token, doc.doc_type) for doc in docs]
    stats_task = batch_get_stats_async(files, user_token)
    meta_task = batch_get_meta_async(docs, user_token)
    stats_dict, metas = await asyncio.gather(stats_task, meta_task)
    meta_dict = {meta.doc_token: meta for meta in metas}
    res = []
    for doc in docs:
        stat = stats_dict.get((doc.doc_token, doc.doc_type))
        meta = meta_dict.get(doc.doc_token)
        if not stat or not meta:
            print(f"Doc failed {vars(doc)}")
            continue
        res.append({
            "title": meta.title,
            "type": meta.doc_type,
            "token": meta.doc_token,
            "node_token": doc.doc_token,
            "source_url": f"https://bytedance.larkoffice.com/{meta.doc_type}/{meta.doc_token}",
            "uv": stat.uv,
            "pv": stat.pv,
            "like_count": max(stat.like_count,0),
            "timestamp": meta.latest_modify_time,
            "uv_today": stat.uv_today,
            "pv_today": stat.pv_today,
            "like_count_today": stat.like_count_today,
            "update_time": meta.latest_modify_time,
        })
        print(f"Doc {vars(meta)}ï¼š{vars(stat)}")
    return res
async def get_children(parent: Node, user_token: str) -> List[Node]:
    all_nodes = []
    page_token = None
    space_id = parent.space_id
    while True:
        # æ„å»ºè¯·æ±‚
        builder = ListSpaceNodeRequest.builder() \
            .space_id(space_id) \
            .page_size(50) \
            .parent_node_token(parent.node_token)
        if page_token:
            builder.page_token(page_token)
            
        request = builder.build()
        options = lark.RequestOption.builder().user_access_token(user_token).build()

        try:
            response: ListSpaceNodeResponse = await client.wiki.v2.space_node.alist(request, options)
            if not response.success():
                logger.error(f"æ‰«æWikiæ–‡æ¡£å¤±è´¥ (space_id: {space_id}, parent: {parent.node_token}): {response.code} {response.msg}")
                break
            
            # å¤„ç†è¿”å›çš„èŠ‚ç‚¹
            if response.data and response.data.items:
                for item in response.data.items:
                    all_nodes.append(item)
                    logger.info(f"{item.title} (token: {item.node_token}, type: {item.obj_type})")
            # å¤„ç†åˆ†é¡µ
            if response.data and response.data.has_more:
                page_token = response.data.page_token
            else:
                break
        except Exception as e:
            logger.exception(f"æ‰«æWikiæ–‡æ¡£å¼‚å¸¸ (space_id: {space_id}, parent: {parent.node_token}): {e}")
            break
    return all_nodes


def parse_doc_url(url: str) -> RequestDoc:
    try:
        # è§£æURL
        parsed = urlparse(url)
        
        # éªŒè¯åŸŸå
        if not parsed.netloc.endswith('larkoffice.com'):
            logger.error(f"æ— æ•ˆçš„é£ä¹¦æ–‡æ¡£URL: '{url}'ï¼ŒåŸŸåå¿…é¡»æ˜¯larkoffice.com")
            return None
            
        # åˆ†å‰²è·¯å¾„éƒ¨åˆ†
        path_parts = [p for p in parsed.path.split('/') if p]
        if len(path_parts) < 2:
            logger.error(f"æ— æ³•ä»URL '{url}' ä¸­è§£æå‡ºæœ‰æ•ˆçš„æ–‡ä»¶ç±»å‹å’ŒToken")
            return None
            
        file_type = path_parts[0]
        file_token = path_parts[1]
        
        return RequestDoc.builder().doc_type(file_type).doc_token(file_token).build()
        
    except Exception as e:
        logger.exception(f"è§£æURLæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return None, None

async def get_document_statistics_async_v2(urls: List[str], user_token: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:        
    docs = [parse_doc_url(url) for url in urls]
    wikis = filter(lambda doc:doc.doc_type == "wiki", docs)
    wiki_tokens = list(map(lambda doc:doc.doc_token, wikis))
    wiki_infos = await get_wiki_info(wiki_tokens, user_token)
    non_wikis = list(filter(lambda doc:doc.doc_type != "wiki", docs))
    doc_infos = await get_doc_info(non_wikis, user_token)
    infos = wiki_infos + doc_infos
    print(infos)
    return infos, None

throttler = Throttler(rate_limit=100, period=60)
async def main():
    user_token = "u-g4K9iOW4l54qLbdLf.fKrv40nWNx14aNVG204kUwwIBb"
    # wiki_tokens = ["GQ0Owf7EmirsookHOh4cpx4XnMf"]
    # stats = await get_wiki_info(wiki_tokens, user_token)
    urls = [
        "https://bytedance.larkoffice.com/wiki/Is5WwlqG1iIkA5kVfwdcVhSMnBe",
        "https://bytedance.larkoffice.com/wiki/MImZwhOfuisCC8kip9icDdcanVb"
    ]
    stats = await get_document_statistics_async_v2(urls, user_token)
    pass

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # root = mock_node("1")
    # qNodes = asyncio.Queue()
    # task = walk_tree(root, mock_get_children, "user_token", qNodes)
    # asyncio.get_event_loop().run_until_complete(task)
    
    asyncio.run(main())
    